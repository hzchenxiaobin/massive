### 2.8 总结

本章提供了 CUDA C 编程模型的简要概述。CUDA C 扩展了 C 语言以支持并行计算。在本章中，我们讨论了这些扩展的一个重要子集。为方便起见，我们总结了本章讨论的扩展内容如下：

#### 2.8.1 函数声明

CUDA C 扩展了 C 函数声明语法，以支持异构并行计算。这些扩展在代码 2.12 中进行了总结。通过使用 "\_\_global\_\_", "\_\_device\_\_" 或 "\_\_host\_\_" 中的一个，CUDA C 程序员可以指示编译器生成kernel函数、device函数或host函数。所有没有这些关键字的函数声明默认为host函数。如果在函数声明中同时使用 "\_\_host\_\_" 和 "\_\_device\_\_"，编译器将生成两个版本的函数，一个用于device，一个用于host。如果函数声明没有任何 CUDA C 扩展关键字，则该函数默认为host函数。

#### 2.8.2 调用kernel函数和启动grid

CUDA C 通过kernel执行配置参数扩展了 C 的函数调用语法，这些参数用 <<< 和 >>> 包围。执行配置参数仅在调用内核函数以启动一个grid时使用。我们讨论了定义grid维度和每个block维度的执行配置参数。有关内核启动扩展以及其他类型的执行配置参数的更多详细信息，读者应参考《CUDA 编程指南》（[NVIDIA, v12.5](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)）。

#### 2.8.3 预定义的内置变量

CUDA 的kernel可以访问一组内置的、预定义的只读变量，这些变量允许每个线程区分自身与其他线程，并确定要处理的数据区域。在本章中，我们讨论了 `threadIdx`、`blockDim` 和 `blockIdx` 变量。在第三章《多维网格和数据》中，我们将讨论使用这些变量的更多细节。

#### 2.8.4 运行时API

CUDA 支持一组为 CUDA C 程序提供服务的 API 函数。在本章中，我们讨论了 `cudaMalloc`、`cudaFree` 和 `cudaMemcpy` 函数。这些函数分别由host代码调用，用于分配global memory、释放global memory以及在host和device之间传输数据。有关其他 CUDA API 函数，读者可以参考《CUDA C 编程指南》。

本章的目标是介绍 CUDA C 的核心概念以及用于编写简单 CUDA C 程序的基本 CUDA 扩展。本章并未全面涵盖所有 CUDA 特性，其中一些特性将在本书的后续章节中讨论。然而，我们将重点放在这些特性所支持的关键并行计算概念上。我们将仅介绍在我们的并行编程技术代码示例中所需的 CUDA C 特性。总的来说，我们鼓励读者随时查阅《CUDA C 编程指南》以获取 CUDA C 特性的更多详细信息。