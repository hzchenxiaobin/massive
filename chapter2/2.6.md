### 2.6 调用kernel函数

```c
int vectAdd(float* A, float* B, float* C, int n) {
  // A_d, B_d, C_d allocations and copies omitted
  ...
  // Launch ceil(n/256) blocks of 256 threads each
  vecAddKernel<<<ceil(n/256.0), 256>>>(A_d, B_d, C_d, n); 
}
```

<figure>
    <style>
     hr {
         border: none;
         height: 2px;
         background-color: black;
         margin: 5px auto;
     }
	</style>
    <figcaption>
        <p class="no-indent" style="font-weight: bold;">
        代码2.12
        </p>
       	<hr style="border: none; height: 2px; background-color: black; margin: 5px auto;">
        <p class="no-indent" style="font-family: 'Arial', 'Helvetica', sans-serif;color: #808080">
            一个调用向量加法kernel函数的语句
        </p>
    </figcaption>
</figure>

在实现了kernel函数之后，剩下的步骤是从主机代码中调用该函数以启动grid。这在代码2.12中有所展示。当host代码调用一个kernel函数时，它通过执行配置参数设置grid和block的维度。配置参数是在传统的C函数参数之前的"<<<"和">>>"之间给出。第一个配置参数指定grid中block的数量。第二个参数指定每个block中的线程数。在这个例子中，每个block中有256个线程。为了确保我们在grid中有足够的线程来覆盖所有的向量元素，我们需要将grid中的block数量设置为所需线程数（在本例中为n）除以线程块大小（在本例中为256）的向上取整（将商向上舍入到最接近的整数值）。有很多方法可以执行向上取整除法。一种方法是对n/256.0应用C中的向上取整函数。使用浮点值256.0确保我们生成一个浮点数值进行除法运算，以便向上取整函数能够正确地将其向上舍入。例如，如果我们想要1000个线程，我们将启动ceil(1000/256.0) = 4个block。因此，该语句将启动4 * 256 = 1024个线程。在核函数中代码2.10所示，使用if (i < n)语句，前1000个线程将对这1000个向量元素执行加法运算，而剩下的24个线程则不会执行。

```c
void vecAdd(float* A, float* B, float* C, int n) {
  float *A_d, *B_d, *C_d;
  int size = n * sizeof(float);
  
  cudaMalloc((void **) &A_d, size);
  cudaMalloc((void **) &B_d, size);
  cudaMalloc((void **) &C_d, size);
  
  cudaMemcpy(A_d, A, size, cudaMemcpyHostToDevice);
  cudaMemcpy(B_d, B, size, cudaMemcpyHostToDevice);
  
  vecAddKernel<<<ceil(n/256.0), 256>>>(A_d, B_d, C_d, n);
  
  cudaMemcpy(C, C_d, size, cudaMemcpyDeviceToHost);
  
  cudaFree(A_d);
  cudaFree(B_d);
  cudaFree(C_d);
}
```

<figure>
    <style>
     hr {
         border: none;
         height: 2px;
         background-color: black;
         margin: 5px auto;
     }
	</style>
    <figcaption>
        <p class="no-indent" style="font-weight: bold;">
        代码2.13
        </p>
       	<hr style="border: none; height: 2px; background-color: black; margin: 5px auto;">
        <p class="no-indent" style="font-family: 'Arial', 'Helvetica', sans-serif;color: #808080">
            完整版本的的host侧vecAdd函数代码
        </p>
    </figcaption>
</figure>
代码2.13展示了vecAdd函数中的最终host代码。这段源代码完成了代码2.5中的骨架。代码2.12和代码2.13共同展示了一个简单的CUDA程序，包含host代码和kerne函数。这段代码每个block固定使用256个线程。[^8]然而，block数量取决于向量的长度（n）。如果n是750，将使用三个block。如果n是4000，将使用16个block。如果n是2,000,000，将使用7813个block。请注意，所有线程块都操作向量的不同部分，它们可以以任意的顺序执行。程序员不能对执行顺序做出任何假设。一个拥有少量执行资源的小型GPU可能只能并行执行一个或两个block。而较大的GPU可能并行执行64或128个block。这使得CUDA kernel函数在硬件上具有可伸缩性，即相同的代码在小型GPU上以较低的速度运行，在大型GPU上以较高的速度运行。我们将在第4章《计算架构与调度》中重新讨论这一点。

需要再次指出的是，使用向量加法示例是因为它的简单性。实际上，分配device内存、将输入数据从host传输到device、将输出数据从device传输到host以及释放device内存的开销可能会使最终的代码比代码2.4中的原始顺序代码更慢。这是因为相对于处理或传输的数据量，内核完成的计算量很小。对于两个浮点输入操作数和一个浮点输出操作数，只执行一次加法操作。实际应用程序中，通常kernel相对于处理的数据量来说需要完成更多工作，从而使额外的开销是值得的。实际应用程序还倾向于在多个kernel调用中将数据保留在device内存中，以便摊销开销。我们将展示几个此类应用程序的例子。



[^8]: 虽然在这个例子中我们使用了任意的块大小256，但块大小应由许多因素决定，这些因素将在后面介绍。
