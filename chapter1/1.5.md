### 1.5 相关的并行编程接口

在过去的几十年里，已经提出了许多并行编程语言和模型（Mattson等，2004）。其中最广泛使用的是用于共享内存多处理器系统的OpenMP（Open，2005）和用于可扩展集群计算的消息传递接口（MPI）（MPI，2009）。这两者都已成为主要计算机供应商支持的标准化编程接口。

OpenMP的实现包括一个编译器和一个运行时系统。程序员向OpenMP编译器指定关于循环的指令（命令）和编译指示（提示）。通过这些指令和编译指示，OpenMP编译器生成并行代码。运行时系统通过管理并行线程和资源来支持并行代码的执行。OpenMP最初是为CPU执行设计的，后来扩展到支持GPU执行。OpenMP的主要优势在于它提供了编译器自动化和运行时支持，从而为程序员抽象出许多并行编程细节。这种自动化和抽象可以帮助应用程序代码在不同供应商生产的系统以及同一供应商的不同代系统之间更具可移植性。我们将这种特性称为性能可移植性( performance portability)。然而，在OpenMP中进行高效编程仍然需要程序员理解所涉及的所有详细的并行编程概念 cv。由于CUDA给予程序员对这些并行编程细节的明确控制，即使对于那些希望使用OpenMP作为主要编程接口的人来说，它也是一个极好的学习工具。此外，根据我们的经验，OpenMP编译器仍在不断发展和改进。许多程序员可能需要在OpenMP编译器不足的部分使用CUDA风格的接口。

另一方面，MPI（[MPI，2009](http://www.mpi-forum.org/docs/mpi-2.2/mpi22-report.pdf)）是一种集群中的计算节点不共享内存的编程接口。所有的数据共享和交互都必须通过显式的消息传递来完成。MPI在高性能计算（HPC）领域被广泛使用。用MPI编写的应用程序已经成功地在拥有超过100,000个节点的集群计算系统上运行。如今，许多HPC集群采用异构CPU/GPU节点。由于计算节点之间缺乏共享内存，将应用程序移植到MPI可能需要相当大的努力。程序员需要进行域分解，以在各个节点之间划分输入和输出数据。基于域分解，程序员还需要调用消息发送和接收函数来管理节点之间的数据交换。相比之下，CUDA为GPU中的并行执行提供了共享内存来解决这个困难。虽然CUDA是每个节点内部的有效接口，但大多数应用程序开发人员需要使用MPI在集群级别进行编程。此外，CUDA通过诸如NVIDIA集体通信库（NCCL）等API，对多GPU编程的支持也在不断增加。因此，HPC领域的并行程序员理解如何在采用多GPU节点的现代计算集群中进行联合MPI/CUDA编程非常重要，这个主题将在第20章"异构计算集群编程"中介绍。

2009年，包括苹果、英特尔、AMD/ATI和NVIDIA在内的几个主要业界参与者共同开发了一个名为开放计算语言（Open Compute Language, OpenCL）的标准化编程模型（[The Khronos Group，2009](http://www.khronos.org/registry/cl/specs/opencl-1.0.29.pdf)）。与CUDA类似，OpenCL编程模型定义了语言扩展和运行时API，允许程序员在大规模并行处理器中管理并行性和数据传递。与CUDA相比，OpenCL更多地依赖API，而较少依赖语言扩展。这使得供应商能够快速调整他们现有的编译器和工具来处理OpenCL程序。OpenCL是一个标准化的编程模型，用OpenCL开发的应用程序可以在所有支持OpenCL语言扩展和API的处理器上正确运行，无需修改。然而，为了在新处理器上实现高性能，可能需要对应用程序进行修改。

些熟悉OpenCL和CUDA的人都知道，OpenCL和CUDA的关键概念和特性之间存在显著的相似性。也就是说，CUDA程序员可以以最小的代价学习OpenCL编程。更重要的是，在使用CUDA时学到的几乎所有技术都可以轻松应用到OpenCL编程中。