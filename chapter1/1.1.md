## 1.1 异构并行计算

​	自2003年以来，半导体行业在设计微处理器时确立了两个主要路径([Hwu et al., 2008](https://www.researchgate.net/publication/260711393_Computer_Architecture_and_Amdahl's_Law))。多核路径旨在转向多核的同时保持顺序程序的执行速度。多核处理器最初从双核处理器开始，随着每一代半导体工艺的进步，核心数量不断增加。最近的一个例子是英特尔的一款多核服务器微处理器，它最多拥有24个处理器核心，每个核心都是一个乱序执行、多指令发射的处理器，实现了完整的386指令集，支持超线程技术（每个核心有两个硬件线程），旨在最大化顺序程序的执行速度。另一个例子是ARM Ampere最近推出的一款多核服务器处理器，拥有128个处理器核心。

​	相比之下，多线程路径更加注重并行应用程序的执行吞吐量。多线程路径始于大量线程，每一代的线程数量也不断增加。最近的一个典型例子是NVIDIA Tesla A100图形处理单元（GPU），它拥有成千上万个线程，在大量简单的顺序管道中执行。自2003年以来，多线程处理器，特别是GPU，一直引领着浮点性能的竞赛。截至2021年，A100 GPU的峰值浮点吞吐量为64位双精度9.7 TFLOPS、32位单精度156 TFLOPS以及16位半精度312 TFLOPS。相比之下，最近的英特尔24核处理器的双精度峰值浮点吞吐量为0.33 TFLOPS，单精度为0.66 TFLOPS。在过去几年中，多线程GPU和多核CPU之间的峰值浮点计算吞吐量比率一直在增加。这些不一定是应用程序的实际执行速度;它们仅仅是这些芯片中执行资源可能支持的原始速度。

​	多核和多线程之间如此巨大的峰值性能差距已经积累成一个显著的"电位差",在某个时刻,必然会有所突破。我们已经到达了那个时刻。迄今为止,这个巨大的峰值性能差距已经促使许多应用程序开发者将其软件的计算密集型部分转移到GPU上执行。也许更重要的是,并行执行性能的大幅提升使得深度学习等本质上由计算密集型部分组成的革命性新应用成为可能。毫不奇怪,这些计算密集型部分也是并行编程的主要目标:当有更多工作要做时,就有更多机会将工作分配给协作的并行工作者,即线程。