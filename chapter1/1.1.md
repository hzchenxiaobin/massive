## 1.1 异构并行计算

​	自2003年以来，半导体行业在设计微处理器时确立了两个主要路径([Hwu et al., 2008](https://www.researchgate.net/publication/260711393_Computer_Architecture_and_Amdahl's_Law))。多核路径旨在转向多核的同时保持顺序程序的执行速度。多核处理器最初从双核处理器开始，随着每一代半导体工艺的进步，核心数量不断增加。最近的一个例子是英特尔的一款多核服务器微处理器，它最多拥有24个处理器核心，每个核心都是一个乱序执行、多指令发射的处理器，实现了完整的386指令集，支持超线程技术（每个核心有两个硬件线程），旨在最大化顺序程序的执行速度。另一个例子是ARM Ampere最近推出的一款多核服务器处理器，拥有128个处理器核心。

​	相比之下，多线程路径更加注重并行应用程序的执行吞吐量。多线程路径始于大量线程，每一代的线程数量也不断增加。最近的一个典型例子是NVIDIA Tesla A100图形处理单元（GPU），它拥有成千上万个线程，在大量简单的顺序管道中执行。自2003年以来，多线程处理器，特别是GPU，一直引领着浮点性能的竞赛。截至2021年，A100 GPU的峰值浮点吞吐量为64位双精度9.7 TFLOPS、32位单精度156 TFLOPS以及16位半精度312 TFLOPS。相比之下，最近的英特尔24核处理器的双精度峰值浮点吞吐量为0.33 TFLOPS，单精度为0.66 TFLOPS。在过去几年中，多线程GPU和多核CPU之间的峰值浮点计算吞吐量比率一直在增加。这些不一定是应用程序的实际执行速度;它们仅仅是这些芯片中执行资源可能支持的原始速度。

​	多核和多线程之间如此巨大的峰值性能差距已经积累成一个显著的"电位差",在某个时刻,必然会有所突破。我们已经到达了那个时刻。迄今为止,这个巨大的峰值性能差距已经促使许多应用程序开发者将其软件的计算密集型部分转移到GPU上执行。也许更重要的是,并行执行性能的大幅提升使得深度学习等本质上由计算密集型部分组成的革命性新应用成为可能。毫不奇怪,这些计算密集型部分也是并行编程的主要目标:当有更多工作要做时,就有更多机会将工作分配给协作的并行工作者,即线程。

One might ask why there is such a large peak performance gap between many-threaded GPUs and multicore CPUs. The answer lies in the differences in the fundamental design philosophies between the two types of processors, as illus trated in Fig. 1.1. The design of a CPU, as shown in Fig. 1.1A, is optimized for sequential code performance. The arithmetic units and operand data delivery logic are designed to minimize the effective latency of arithmetic operations at the cost of increased use of chip area and power per unit. Large last-level on-chip caches are designed to capture frequently accessed data and convert some of the long latency memory accesses into short-latency cache accesses. Sophisticated branch prediction logic and execution control logic are used to mitigate the latency of conditional branch instructions. By reducing the latency of operations, the CPU hardware reduces the execution latency of each individual thread. However, the low-latency arithmetic units, sophisticated operand delivery logic, large cache memory, and control logic consume chip area and power that could otherwise be used to provide more arithmetic execution units and memory access channels. This design approach is commonly referred to as latency-oriented design.

​	有人可能会问，为什么多线程GPU和多核CPU之间的峰值性能差距如此之大。答案在于这两类处理器在基本设计理念上的差异，如图1.1所示。图1.1A显示了CPU的设计，其优化目标是顺序代码的性能。算术单元和操作数数据传递逻辑的设计旨在以增加每单位芯片面积和功耗为代价，尽量减少算术运算的有效延迟。大型末级片上缓存被设计用来捕获经常访问的数据，并将一些长延迟的内存访问转换为短延迟的缓存访问。复杂的分支预测逻辑和执行控制逻辑用于减轻条件分支指令的延迟。通过减少操作的延迟，CPU硬件减少了每个单独线程的执行延迟。然而，低延迟的算术单元、复杂的操作数传递逻辑、大容量的缓存内存和控制逻辑消耗了芯片面积和功率，而这些本可以用于提供更多的算术执行单元和内存访问通道。这种设计方法通常被称为面向延迟的设计。

[![图1.1](https://github.com/hzchenxiaobin/massive/blob/master/pic/chapter1/fig1.1.png)]





