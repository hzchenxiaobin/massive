## 1.1 异构并行计算

​	自2003年以来，半导体行业在设计微处理器时确立了两个主要路径([Hwu et al., 2008](https://www.researchgate.net/publication/260711393_Computer_Architecture_and_Amdahl's_Law))。多核路径旨在转向多核的同时保持顺序程序的执行速度。多核处理器最初从双核处理器开始，随着每一代半导体工艺的进步，核心数量不断增加。最近的一个例子是英特尔的一款多核服务器微处理器，它最多拥有24个处理器核心，每个核心都是一个乱序执行、多指令发射的处理器，实现了完整的386指令集，支持超线程技术（每个核心有两个硬件线程），旨在最大化顺序程序的执行速度。另一个例子是ARM Ampere最近推出的一款多核服务器处理器，拥有128个处理器核心。

​	相比之下，多线程路径更加注重并行应用程序的执行吞吐量。多线程路径始于大量线程，每一代的线程数量也不断增加。最近的一个典型例子是NVIDIA Tesla A100图形处理单元（GPU），它拥有成千上万个线程，在大量简单的顺序管道中执行。自2003年以来，多线程处理器，特别是GPU，一直引领着浮点性能的竞赛。截至2021年，A100 GPU的峰值浮点吞吐量为64位双精度9.7 TFLOPS、32位单精度156 TFLOPS以及16位半精度312 TFLOPS。相比之下，最近的英特尔24核处理器的双精度峰值浮点吞吐量为0.33 TFLOPS，单精度为0.66 TFLOPS。在过去几年中，多线程GPU和多核CPU之间的峰值浮点计算吞吐量比率一直在增加。这些不一定是应用程序的实际执行速度;它们仅仅是这些芯片中执行资源可能支持的原始速度。

​	多核和多线程之间如此巨大的峰值性能差距已经积累成一个显著的"电位差",在某个时刻,必然会有所突破。我们已经到达了那个时刻。迄今为止,这个巨大的峰值性能差距已经促使许多应用程序开发者将其软件的计算密集型部分转移到GPU上执行。也许更重要的是,并行执行性能的大幅提升使得深度学习等本质上由计算密集型部分组成的革命性新应用成为可能。毫不奇怪,这些计算密集型部分也是并行编程的主要目标:当有更多工作要做时,就有更多机会将工作分配给协作的并行工作者,即线程。

​	有人可能会问，为什么多线程GPU和多核CPU之间的峰值性能差距如此之大。答案在于这两类处理器在基本设计理念上的差异，如图1.1所示。图1.1A显示了CPU的设计，其优化目标是顺序代码的性能。算术单元和操作数数据传递逻辑的设计旨在尽量减少算术运算的有效延迟，以增加每单位芯片面积和功耗为代价。大型末级片上缓存被设计用来捕获经常访问的数据，并将一些长延迟的内存访问转换为短延迟的缓存访问。复杂的分支预测逻辑和执行控制逻辑用于缓解条件分支指令的延迟。通过降低操作的延迟,CPU硬件减少了每个单独线程的执行延迟。然而，低延迟的算术单元、复杂的操作数传递逻辑、大容量的缓存内存和控制逻辑消耗了芯片面积和功率，而这些本可以用于提供更多的算术执行单元和内存访问通道。这种设计方法通常被称为面向延迟的设计。

<figure>
    <style>
     hr {
         border: none;
         height: 2px;
         background-color: black;
         margin: 5px auto;
     }
	</style>
    <img src="..\pic\chapter1\fig1.1.png" alt="图1.1">
    <figcaption>
        <p style="font-weight: bold;">
        图1.1
        </p>
       	<hr style="border: none; height: 2px; background-color: black; margin: 5px auto;">
        <p style="font-family: 'Arial', 'Helvetica', sans-serif;color: #808080">
            CPU和GPU有着根本不同的设计理念：（A）CPU设计是面向延迟的；（B）GPU设计是面向吞吐量的。
        </p>
    </figcaption>
</figure>
  另一方面，GPU的设计理念受到快速发展的电子游戏行业的影响，该行业对每帧视频中执行大量浮点计算和内存访问的能力施加了巨大的经济压力。这种需求促使GPU厂商寻找方法，最大限度地利用芯片面积和功耗预算来专注于浮点计算和内存访问吞吐量。

在图形应用中,为执行视点变换和物体渲染等任务而需要每秒进行大量浮点计算,这一需求是相当直观的。此外,每秒执行大量内存访问的需求同样重要,甚至可能更为重要。许多图形应用的速度受限于数据从内存系统传输到处理器(反之亦然)的速率。GPU必须能够在其DRAM（动态随机存取存储器）的图形帧缓冲区中高速移动极大量的数据，因为这种数据移动使视频显示效果更加丰富并且满足游戏玩家的需求。游戏应用普遍接受的宽松内存模型（系统软件、应用程序和I/O设备期望其内存访问方式的模型）也使得GPU更容易支持大规模的并行内存访问。

相比之下，通用处理器必须满足来自传统操作系统、应用程序和I/O设备的要求，这些要求对支持并行内存访问提出了更多挑战，因此增加内存访问吞吐量（通常称为内存带宽）变得更加困难。结果是，图形芯片的内存带宽大约是同时期可用的CPU芯片的10倍左右，我们预计在相当长一段时间内，GPU在内存带宽方面仍将保持优势。

相比增加吞吐量，降低延迟在功耗和芯片面积方面要昂贵得多。例如，通过将算术单元的数量翻倍，可以将算术吞吐量翻倍，代价是芯片面积和功耗翻倍。然而，将算术延迟减半可能需要将电流翻倍，代价是使用的芯片面积增加一倍以上，功耗增加四倍。因此，GPU中普遍采用的解决方案是优化大量线程的执行吞吐量，而不是降低单个线程的延迟。这种设计方法通过允许流水线内存通道和算术运算具有较长的延迟来节省芯片面积和功耗。内存访问硬件和算术单元在面积和功耗上的减少，使GPU设计师能够在一个芯片上放置更多的这些单元，从而提高总体执行吞吐量。图1.1直观地说明了设计方法的差异，其中图1.1A展示了CPU设计中较少数量的较大算术单元和较少数量的内存通道，与之相比，图1.1B展示了较多数量的较小算术单元和较多数量的内存通道。

这些GPU的应用软件预期会以大量并行线程的方式编写。硬件利用大量线程的优势，在某些线程等待长延迟内存访问或算术操作时找到可以执行的工作。图1.1B中的小型缓存内存用于帮助控制这些应用程序的带宽需求，使得访问相同内存数据的多个线程不需要全都去访问DRAM。这种设计风格通常被称为面向吞吐量的设计，因为它致力于最大化大量线程的总执行吞吐量，同时允许单个线程可能需要更长的时间来执行。

显然，GPU被设计为并行的、面向吞吐量的计算引擎，它们在某些任务上的表现不会如CPU那样出色。对于只有一个或很少线程的程序，具有较低操作延迟的CPU可以实现比GPU更高的性能。当一个程序有大量线程时，具有较高执行吞吐量的GPU可以实现比CPU更高的性能。因此，人们应预期许多应用程序会同时使用CPU和GPU，在CPU上执行顺序部分，在GPU上执行数值密集型部分。这就是为什么NVIDIA在2007年引入的计算统一设备架构（CUDA）编程模型被设计用来支持应用程序的CPU-GPU联合执行。

当应用程序开发者选择运行其应用程序的处理器时，速度并不是唯一的决定因素。其他几个因素甚至可能更为重要。首先也是最重要的，所选择的处理器必须在市场上有非常大的存在量，这被称为处理器的安装基数(installed base)。原因很简单。软件开发的成本最好是通过非常庞大的客户群来证明其合理性。市场存量较小的处理器上运行的应用程序将没有大的客户基础。这一直是传统并行计算系统的主要问题之一，与通用微处理器相比，它们的市场存在量可以忽略不计。只有少数由政府和大公司资助的精英应用程序在这些传统并行计算系统上成功开发。这一情况在多线程GPU出现后发生了变化。由于在PC市场的普及，GPU已经售出数亿台。实际上所有的台式PC和高端笔记本电脑都配有GPU。截至目前，已有超过10亿台支持CUDA的GPU在使用。如此庞大的市场存量在经济上对开发者有巨大的吸引力。

另一个重要的决策因素是实际的外形尺寸和易于访问性。在2006年之前，并行软件应用程序运行在数据中心服务器或部门集群上。但是这种执行环境往往限制了这些应用程序的使用。例如，在医疗成像这样的应用程序中，基于64节点集群机器发表论文是可以的。但实际的临床应用，如在磁共振成像（MRI）机器上，则是基于PC和一些特殊硬件加速器的组合。原因很简单，像GE和西门子这样的制造商无法在临床环境中销售需要成架的计算机服务器箱的MRI，而这在学术部门中很常见。事实上，国立卫生研究院（NIH）曾一度拒绝资助并行编程项目；他们认为并行软件的影响将会受限，因为巨大的基于集群的机器在临床环境中无法工作。今天，许多公司出厂的MRI产品都配备了GPU，NIH也为使用GPU计算的研究提供资金。



