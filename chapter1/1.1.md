## 1.1 异构并行计算

​	自2003年以来，半导体行业在设计微处理器时确立了两个主要路径([Hwu et al., 2008](https://www.researchgate.net/publication/260711393_Computer_Architecture_and_Amdahl's_Law))。多核路径旨在转向多核的同时保持顺序程序的执行速度。多核处理器最初从双核处理器开始，随着每一代半导体工艺的进步，核心数量不断增加。最近的一个例子是英特尔的一款多核服务器微处理器，它最多拥有24个处理器核心，每个核心都是一个乱序执行、多指令发射的处理器，实现了完整的386指令集，支持超线程技术（每个核心有两个硬件线程），旨在最大化顺序程序的执行速度。另一个例子是ARM Ampere最近推出的一款多核服务器处理器，拥有128个处理器核心。

​	相比之下，多线程路径更加注重并行应用程序的执行吞吐量。多线程路径始于大量线程，每一代的线程数量也不断增加。最近的一个典型例子是NVIDIA Tesla A100图形处理单元（GPU），它拥有成千上万个线程，在大量简单的顺序管道中执行。自2003年以来，多线程处理器，特别是GPU，一直引领着浮点性能的竞赛。截至2021年，A100 GPU的峰值浮点吞吐量为64位双精度9.7 TFLOPS、32位单精度156 TFLOPS以及16位半精度312 TFLOPS。相比之下，最近的英特尔24核处理器的双精度峰值浮点吞吐量为0.33 TFLOPS，单精度为0.66 TFLOPS。在过去几年中，多线程GPU和多核CPU之间的峰值浮点计算吞吐量比率一直在增加。这些不一定是应用程序的实际执行速度;它们仅仅是这些芯片中执行资源可能支持的原始速度。

​	多核和多线程之间如此巨大的峰值性能差距已经积累成一个显著的"电位差",在某个时刻,必然会有所突破。我们已经到达了那个时刻。迄今为止,这个巨大的峰值性能差距已经促使许多应用程序开发者将其软件的计算密集型部分转移到GPU上执行。也许更重要的是,并行执行性能的大幅提升使得深度学习等本质上由计算密集型部分组成的革命性新应用成为可能。毫不奇怪,这些计算密集型部分也是并行编程的主要目标:当有更多工作要做时,就有更多机会将工作分配给协作的并行工作者,即线程。

​	有人可能会问，为什么多线程GPU和多核CPU之间的峰值性能差距如此之大。答案在于这两类处理器在基本设计理念上的差异，如图1.1所示。图1.1A显示了CPU的设计，其优化目标是顺序代码的性能。算术单元和操作数数据传递逻辑的设计旨在尽量减少算术运算的有效延迟，以增加每单位芯片面积和功耗为代价。大型末级片上缓存被设计用来捕获经常访问的数据，并将一些长延迟的内存访问转换为短延迟的缓存访问。复杂的分支预测逻辑和执行控制逻辑用于缓解条件分支指令的延迟。通过降低操作的延迟,CPU硬件减少了每个单独线程的执行延迟。然而，低延迟的算术单元、复杂的操作数传递逻辑、大容量的缓存内存和控制逻辑消耗了芯片面积和功率，而这些本可以用于提供更多的算术执行单元和内存访问通道。这种设计方法通常被称为面向延迟的设计。

<figure>
    <style>
     hr {
         border: none;
         height: 2px;
         background-color: black;
         margin: 5px auto;
     }
	</style>
    <img src="..\pic\chapter1\fig1.1.png" alt="图1.1">
    <figcaption>
        <p style="font-weight: bold;">
        图1.1
        </p>
       	<hr style="border: none; height: 2px; background-color: black; margin: 5px auto;">
        <p style="font-family: 'Arial', 'Helvetica', sans-serif;color: #808080">
            CPU和GPU有着根本不同的设计理念：（A）CPU设计是面向延迟的；（B）GPU设计是面向吞吐量的。
        </p>
    </figcaption>
</figure>
  另一方面，GPU的设计理念受到快速发展的电子游戏行业的影响，该行业对每帧视频中执行大量浮点计算和内存访问的能力施加了巨大的经济压力。这种需求促使GPU厂商寻找方法，最大限度地利用芯片面积和功耗预算来专注于浮点计算和内存访问吞吐量。

在图形应用中,为执行视点变换和物体渲染等任务而需要每秒进行大量浮点计算,这一需求是相当直观的。此外,每秒执行大量内存访问的需求同样重要,甚至可能更为重要。许多图形应用的速度受限于数据从内存系统传输到处理器(反之亦然)的速率。GPU必须能够在其DRAM（动态随机存取存储器）的图形帧缓冲区中高速移动极大量的数据，因为这种数据移动使视频显示效果更加丰富并且满足游戏玩家的需求。游戏应用普遍接受的宽松内存模型（系统软件、应用程序和I/O设备期望其内存访问方式的模型）也使得GPU更容易支持大规模的并行内存访问。

相比之下，通用处理器必须满足来自传统操作系统、应用程序和I/O设备的要求，这些要求对支持并行内存访问提出了更多挑战，因此增加内存访问吞吐量（通常称为内存带宽）变得更加困难。结果是，图形芯片的内存带宽大约是同时期可用的CPU芯片的10倍左右，我们预计在相当长一段时间内，GPU在内存带宽方面仍将保持优势。

相比增加吞吐量，降低延迟在功耗和芯片面积方面要昂贵得多。例如，通过将算术单元的数量翻倍，可以将算术吞吐量翻倍，代价是芯片面积和功耗翻倍。然而，将算术延迟减半可能需要将电流翻倍，代价是使用的芯片面积增加一倍以上，功耗增加四倍。因此，GPU中普遍采用的解决方案是优化大量线程的执行吞吐量，而不是降低单个线程的延迟。这种设计方法通过允许流水线内存通道和算术运算具有较长的延迟来节省芯片面积和功耗。内存访问硬件和算术单元在面积和功耗上的减少，使GPU设计师能够在一个芯片上放置更多的这些单元，从而提高总体执行吞吐量。图1.1直观地说明了设计方法的差异，其中图1.1A展示了CPU设计中较少数量的较大算术单元和较少数量的内存通道，与之相比，图1.1B展示了较多数量的较小算术单元和较多数量的内存通道。

这些GPU的应用软件预期会以大量并行线程的方式编写。硬件利用大量线程的优势，在某些线程等待长延迟内存访问或算术操作时找到可以执行的工作。图1.1B中的小型缓存内存用于帮助控制这些应用程序的带宽需求，使得访问相同内存数据的多个线程不需要全都去访问DRAM。这种设计风格通常被称为面向吞吐量的设计，因为它致力于最大化大量线程的总执行吞吐量，同时允许单个线程可能需要更长的时间来执行。





