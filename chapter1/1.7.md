### 1.7 本书的组织结构

本书分为四个部分。第一部分涵盖了并行编程、数据并行性、GPU和性能优化的基本概念。这些基础章节为读者提供了成为GPU程序员所必需的基本知识和技能。第二部分介绍了基本的并行模式，第三部分则涵盖了更高级的并行模式和应用程序。这两个部分在第一部分学到的知识和技能的基础上进行深入，根据需要引入其他GPU架构特性和优化技术。最后一部分，即第四部分，介绍了高级实践，以完善那些希望成为专业GPU程序员的读者的知识。

第一部分关于基本概念的内容包括第2至第6章。第2章《异构数据并行计算》介绍了数据并行性和CUDA C编程。本章假设读者已经有C编程的经验。首先将CUDA C作为C语言的一个简单、小型扩展进行介绍，它支持异构CPU/GPU计算和广泛使用的单程序、多数据并行编程模型。接下来，本章涵盖了以下涉及的思维过程：(1) 确定应用程序中需要并行化的部分，(2) 隔离并行代码所需使用的数据，使用API函数在并行计算设备上分配内存，(3) 使用API函数将数据传输到并行计算设备，(4) 将并行部分开发成由并行线程执行的内核函数，(5) 启动内核函数以由并行线程执行，(6) 最终使用API函数调用将数据传回主处理器。我们用一个矢量加法的实例来说明这些概念。虽然本章的目标是教授足够的CUDA C编程模型概念，使读者能够编写一个简单的并行CUDA C程序，但它涵盖了基于任何并行编程接口开发并行应用程序所需的几项基本技能。

第3章《多维网格和数据》详细介绍了CUDA的并行执行模型，特别是如何使用多维线程组织处理多维数据。本章提供了有关线程的创建、组织、资源绑定和数据绑定的深入见解，使读者能够使用CUDA C实现复杂的计算。

第4章《计算架构和调度》介绍了GPU架构，重点讲述计算核心的组织方式以及线程在这些核心上的调度执行。本章讨论了各种架构因素及其对GPU架构上执行的代码性能的影响。这些内容包括透明可扩展性、SIMD执行和控制分歧、多线程和延迟容忍度以及占用率等概念，所有这些概念都在本章中进行了定义和讨论。

第5章《内存架构和数据局部性》在第4章《计算架构和调度》的基础上，讨论了GPU的内存架构。本章还讨论了用于存储CUDA变量的特殊内存，这些内存可以用于管理数据传输并提高程序执行速度。我们介绍了分配和使用这些内存的CUDA语言特性。适当使用这些内存可以显著提高数据访问吞吐量，并有助于缓解内存系统中的流量拥堵问题。

第6章《性能考量》介绍了当前CUDA硬件中几个重要的性能考量因素。特别是，它详细描述了线程执行和内存访问的理想模式。这些细节构成了程序员思考其计算和数据组织决策后果的概念基础。本章最后列出了GPU程序员常用的常见优化策略清单，用于优化任何计算模式。这份清单将在本书接下来的两个部分中用来优化各种并行模式和应用程序。

第二部分关于基本并行模式的内容包括第7至第12章。第7章《卷积》介绍了卷积，这是一种常用的并行计算模式，源于数字信号处理和计算机视觉，要求对数据访问局部性进行精细管理。我们还利用这个模式介绍现代GPU中的常量内存和缓存。第8章《模板》介绍了模板，这种模式类似于卷积，但源于求解微分方程，并具有特定的特征，提供了进一步优化数据访问局部性的独特机会。我们还利用这个模式介绍线程和数据的3D组织，并展示了第6章《性能考量》中介绍的、针对线程粒度的优化。

第9章《并行直方图》介绍了直方图，这是一种广泛用于统计数据分析和大数据集模式识别的模式。我们还利用这个模式介绍原子操作，作为协调对共享数据的并发更新的一种方法，以及减少这些操作开销的私有化优化。第10章《归约与最小化分歧》介绍了归约树模式，用于汇总输入数据集合。我们还利用这个模式展示了控制分歧对性能的影响，并展示了减轻这种影响的技术。第11章《前缀和（扫描）》介绍了前缀和或扫描，这是一种重要的并行计算模式，可以将本质上是顺序的计算转换为并行计算。我们还利用这个模式介绍并行算法中的工作效率概念。最后，第12章《合并》介绍了并行合并，这是一种在分而治之工作分配策略中广泛使用的模式。我们还在本章中介绍了动态输入数据识别和组织。

第三部分关于高级并行模式和应用程序的内容与第二部分类似，但所涵盖的模式更为复杂，通常包括更多的应用背景。因此，这些章节更注重应用特定的考量，而不是介绍新技术或新特性。对于每个应用程序，我们首先确定构建并行执行基本结构的可选方法，并分析每种方法的优缺点。然后，我们详细讲解实现高性能所需的代码转换步骤。这些章节帮助读者将前面章节的所有材料结合起来，并支持他们进行自己的应用程序开发项目。

第三部分包括第13至第19章。第13章《排序》介绍了两种并行排序形式：基数排序和归并排序。这种高级模式利用了前面章节中介绍的更基本的模式，特别是前缀和和并行归并。第14章《稀疏矩阵计算》介绍了稀疏矩阵计算，这种计算广泛用于处理非常大的数据集。本章向读者介绍了为了更高效的并行访问而重新排列数据的概念：数据压缩、填充、排序、转置和规范化。第15章《图遍历》介绍了图算法以及如何在GPU编程中高效实现图搜索。章节中提出了许多不同的策略来并行化图算法，并讨论了图结构对选择最佳算法的影响。这些策略建立在更基本的模式之上，例如直方图和归并。

第16章《深度学习》介绍了深度学习，这是GPU计算中一个越来越重要的领域。我们介绍了卷积神经网络的高效实现，并将更深入的讨论留给其他资料。卷积神经网络的高效实现利用了如tiling和卷积模式等技术。第17章《迭代磁共振成像重建》介绍了非笛卡尔MRI重建，以及如何利用循环融合和散射到聚集转换等技术来增强并行性并减少同步开销。第18章《静电势图》介绍了分子可视化和分析，这些过程受益于处理不规则数据的技术，并应用从稀疏矩阵计算中学到的经验。

第19章《并行编程与计算思维》介绍了计算思维，即以更有利于高性能计算的方式制定和解决计算问题的艺术。本章涵盖了组织程序的计算任务，使其能够并行执行的概念。我们从讨论将抽象科学和问题特定概念转化为计算任务的翻译过程开始，这是生成高质量应用软件（串行或并行）的重要第一步。本章还讨论了并行算法结构及其对应用性能的影响，这些讨论基于对CUDA性能调优经验的实践。虽然我们没有详细介绍这些替代并行编程风格的实现细节，但我们期望读者能够在本书所获得的基础上学会在任何其中一种风格下进行编程。此外，我们还提供了一个高层次的案例研究，展示通过创造性的计算思维所能带来的机遇。

第四部分关于高级实践包括第20至第22章。第20章《编程异构计算集群》涵盖了在异构计算集群上进行CUDA编程，其中每个计算节点都包括CPU和GPU。我们讨论了在CUDA中使用MPI来整合节点间计算和节点内计算，以及由此引发的通信问题和实践经验。第21章《CUDA动态并行性》介绍了动态并行性，这是GPU根据数据或程序结构动态创建工作的能力，而不总是等待CPU执行。第22章《高级实践与未来发展》列出了一系列CUDA程序员需要了解的高级特性和实践，包括零拷贝内存、统一虚拟内存、多个内核的同时执行、函数调用、异常处理、调试、性能分析、双精度支持、可配置的缓存/临时存储器大小等等。例如，早期版本的CUDA在CPU和GPU之间提供了有限的共享内存能力，程序员需要显式管理数据在CPU和GPU之间的传输。然而，当前版本的CUDA支持统一虚拟内存和零拷贝内存等特性，使得在CPU和GPU之间无缝共享数据成为可能。借助这些支持，CUDA程序员可以将变量和数据结构声明为CPU和GPU之间共享。运行时硬件和软件维护一致性，并根据需求自动执行优化的数据传输操作，显著降低了在重叠数据传输与计算和I/O活动中涉及的编程复杂性。在教材的开头部分，我们使用显式数据传输的API，让读者更好地理解底层发生的情况。随后在第22章《高级实践与未来发展》中介绍了统一虚拟内存和零拷贝内存。

虽然本书各章节基于CUDA，但它们帮助读者建立了并行编程的基础。我们认为，人们最好通过具体示例来理解。换句话说，我们必须首先在特定编程模型的上下文中学习概念，这为我们在将知识推广到其他编程模型时提供了坚实的基础。在此过程中，我们可以借助CUDA示例的具体经验。深入了解CUDA也使我们能够获得成熟度，这有助于我们学习甚至可能与CUDA模型无关的概念。

第23章《总结与展望》提供了总结性的评论和对大规模并行编程未来的展望。我们首先重新审视我们的目标，并总结各章如何共同努力以实现这些目标。随后，我们得出结论，预测大规模并行计算的快速进展将使其成为未来十年中最激动人心的领域之一。