### 4.9 总结

GPU 被组织成多个 SM，每个 SM 包含多个处理核心block，这些block共享控制逻辑和内存资源。当一个grid被启动时，block会以任意顺序分配给 SM，从而实现 CUDA 应用程序的透明可扩展性。这种透明的可扩展性带来了一定的限制：不同block中的线程无法相互同步。

线程按block分配给 SM 进行执行。一旦一个block被分配给一个 SM，它会进一步被划分为多个 warps。warp 中的线程按照 SIMD（单指令多数据）模型执行。如果同一个 warp 中的线程由于采取不同的执行路径而发生分歧，处理block会按路径分批次执行这些路径，每个线程只在对应其所采取路径的批次中处于活动状态。