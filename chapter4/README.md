# 第四章 计算架构和调度

### 章节大纲

<hr>

[4.1 现代GPU的架构](4.1.md)

[4.2 block调度](4.2.md)

[4.3 同步和透明可扩展性](4.3.md)

[4.4 Warp和SIMD硬件](4.4.md)

[4.5 控制分歧](4.5.md)

[4.6 Warp调度和延迟容忍](4.6.md)

[4.7 资源分区和占用](4.7.md)

[4.8 查询设备属性](4.8.md)

[4.9 总结](4.9.md)

[练习题](exercise.md)

[习题答案](exercise_answer.md)

[参考文献](reference.md)

在第1章“引言”中，我们了解到CPU的设计旨在将指令的延迟最小化，而GPU的设计旨在将指令执行的吞吐量最大化。在第2章“异构数据并行计算”和第3章“多维网格与数据”中，我们学习了CUDA编程接口的核心特性，用于创建和调用kernel以启动和执行线程。在接下来的三章中，我们将讨论现代GPU的架构，包括计算架构和内存架构，以及基于对这些架构理解的性能优化技术。本章介绍了GPU计算架构的几个方面，这些方面对于CUDA C程序员理解和推理其kernel代码的性能行为至关重要。我们将首先展示计算架构的高层次简化视图，并探讨灵活的资源分配、block调度和占用的概念。然后我们将深入讨论线程调度、延迟容忍、控制分歧和同步。最后，我们将描述一些API函数，这些函数可以用来查询GPU中可用的资源，并提供帮助估算执行内核时GPU占用情况的工具。在接下来的两章中，我们将介绍GPU内存架构的核心概念和编程注意事项。具体而言，第5章“内存架构和数据局部性”将重点介绍片上内存架构，第6章“性能考虑”将简要介绍片外内存架构，并详细阐述整个GPU架构的各种性能考虑。掌握这些概念的CUDA C程序员将具备编写和理解高性能并行内核的能力。





