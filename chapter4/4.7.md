### 4.7 资源分区和占用

我们已经看到，为了容忍长延迟操作，向SM分配多个warp是理想的。然而，并不总是能够向SM分配其支持的最大数量的warp。分配给一个SM的warp数量与其支持的最大数量的比率称为占用率（occupancy）。要理解是什么阻止了SM达到最大占用率，首先重要的是要了解SM资源是如何划分的。

SM的执行资源包括寄存器、共享内存（在第5章“内存架构和数据局部性”中讨论）、block slot和线程slot。这些资源在线程之间动态分配以支持其执行。例如，一个Ampere A100 GPU每个SM最多可以支持32个block、64个warp（2048个线程）和每个block1024个线程。如果以最大允许的block大小1024线程启动一个grid，每个SM中的2048个线程slot将被划分并分配给2个块。在这种情况下，每个SM可以容纳最多2个block。类似地，如果以512、256、128或64线程的block大小启动一个grid，这2048个线程slot将分别被划分并分配给4、8、16或32个block。

这种在block之间动态划分线程slot的能力使得SM具有多功能性。它们既可以执行多个每个包含少量线程的block，也可以执行少量每个包含大量线程的block。这种动态划分与固定划分方法形成对比，固定划分方法中每个block会接收固定数量的资源，而不管其实际需求如何。固定划分会导致当一个block需要的线程少于固定划分支持的数量时浪费线程slot，同时无法支持需要更多线程slot的block。

资源的动态划分可能导致资源限制之间的微妙相互作用，从而导致资源的未充分利用。这种相互作用可能发生在block slot和线程slot之间。以Ampere A100为例，我们看到block大小可以从1024变到64，分别导致每个SM有2到32个block。在所有这些情况下，分配给SM的线程总数为2048，这最大化了占用率。然而，考虑每个block有32个线程的情况。在这种情况下，2048个线程slot需要划分并分配给64个block。然而，Volta的SM一次只能支持32个block slot。这意味着只有1024个线程slot会被利用，即32个block，每个block有32个线程。在这种情况下，占用率为(1024个分配的线程)/(2048个最大线程) = 50%。因此，为了充分利用线程slot并实现最大占用率，每个block至少需要64个线程。

另一种可能对占用率产生负面影响的情况是每个block的最大线程数不能被block大小整除。以Ampere A100为例，我们看到每个SM最多可以支持2048个线程。然而，如果选择768的block大小，SM将只能容纳2个block（1536个线程），留下512个线程slot未被利用。在这种情况下，既没有达到每个SM的最大线程数，也没有达到每个SM的最大块数。此时的占用率为（1536个分配的线程）/（2048个最大线程）= 75%。

前面的讨论没有考虑其他资源限制的影响，比如寄存器和共享内存。我们将在第5章“内存架构和数据局部性”中看到，CUDA内核中声明的自动变量会被放入寄存器中。一些kernel可能会使用很多自动变量，而另一些则使用很少。因此，一些kernel的每个线程需要很多寄存器，而一些kernel的每个线程需要的寄存器很少。通过在SM中动态划分线程间的寄存器，如果每个线程需要的寄存器少，SM可以容纳更多的block；如果每个线程需要的寄存器多，SM可以容纳的block就更少。

然而，需要注意寄存器资源限制对占用率的潜在影响。例如，Ampere A100 GPU每个SM最多允许65,536个寄存器。为了实现完全占用率，每个SM需要为2048个线程提供足够的寄存器，这意味着每个线程不应使用超过(65,536个寄存器)/(2048个线程) = 32个寄存器。比如，如果一个内核每个线程使用64个寄存器，那么65,536个寄存器最多支持1024个线程。在这种情况下，无论block大小设置为多少，该内核都无法达到完全占用率。相反，占用率最多为50%。在某些情况下，编译器可能会执行寄存器溢出操作，以减少每个线程的寄存器需求，从而提高占用率。然而，这通常会增加线程访问溢出寄存器值的时间，可能导致grid的总执行时间增加。类似的分析也适用于第5章“内存架构和数据局部性”中的共享内存资源。

假设程序员实现了一个每线程使用31个寄存器的kernel，并将其配置为每个block512个线程。在这种情况下，SM将有(2048线程)/(512线程/block) = 4个block同时运行。这些线程将总共使用(2048线程) * (31寄存器/线程) = 63,488个寄存器，少于65,536个寄存器的限制。现在假设程序员在kernel中声明了另外两个自动变量，使每个线程使用的寄存器数量增加到33。2048个线程现在需要67,584个寄存器，超过了寄存器限制。CUDA运行时系统可能会通过每个SM只分配3个block而不是4个来处理这种情况，从而将所需的寄存器数量减少到50,688个。然而，这将使每个SM运行的线程数量从2048减少到1536；也就是说，由于使用了两个额外的自动变量，程序的占用率从100%降低到了75%。这有时被称为“性能悬崖”，即资源使用量的轻微增加可能导致并行性和性能的大幅下降([Ryoo等，2008](https://web.eecs.umich.edu/~mahlke/courses/583f11/reading/ryoo_cgo08.pdf))。

显而易见，所有动态划分资源的限制以复杂的方式相互作用。准确确定每个SM中运行的线程数量可能是困难的。建议读者参考CUDA占用率计算器（[CUDA Occupancy Calculator](https://developer.download.nvidia.com/compute/cuda/4_0/sdk/docs/CUDA_Occupancy_Calculator.xls), Web），这是一种可下载的电子表格，可以根据内核对资源的使用情况计算特定设备实现中每个SM上实际运行的线程数量。

